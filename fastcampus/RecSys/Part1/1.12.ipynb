{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Square Error (RMSE)\n",
    "\n",
    "> - 평균 제곱근 편차\n",
    "> - 실제 값과 모델의 예측 값의 차이를 하나의 숫자로 나타낸다.\n",
    "> - 예측 대상 값에 영향을 받는다(Scale-dependent)\n",
    "    - 같은 0.01의 에러값도 어떤 y_pred와 y_actual을 사용했느냐에 따라 다른 의미를 갖는다.\n",
    "> - $$ RMSE = \\sqrt{\\frac{1}{n}\\sum_ {i=1}^n (y_i - \\hat{y_i})^2} $$\n",
    "\n",
    "> - 평점 등 prediction problem의 추천 성능을 평가할 때 사용하는 지표\n",
    "> - 제곱을 하여 더 큰 오차를 만들고, 제곰근(root)으로 원래 scale의 의미있는 숫자로 돌아감\n",
    "> - RMSE는 낮을 수록 추천 알고리즘이 성능이 더 좋다고 정량적으로 평가 가능\n",
    "> - 잔차(residual)의 제곱을 산술평균한 값의 제곱근 = 표준편차 = RMSE\n",
    "    - 관측값들의 상호간 편차 = 관측값과 실제값 사이의 오차\n",
    "    \n",
    "### Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "> - 랭킹 추천에 많이 사용되는 평가 지표\n",
    "> - 기존 정보검색에서 많이 사용되는 지표\n",
    "> - Top-N 랭킹 리스트를 만들고, 더 관심있거나 관련성 높은 아이템 포함 여부를 평가\n",
    "> - 순위에 가중치를 주고, 단순한 랭킹이 아닌 데이터의 성향을 반영하기 위한 평가 지표\n",
    "> - MAP(Mean Average Precision), Top K Precision / Recall 등 평가방법 보안\n",
    "    - 추천 또는 정보검색에서 특정 아이템에 biased된 경우\n",
    "    - 이미 유명하고 잘 알려진 인기있는 아이템 또는 한 명의 사용자에 의해서 만들어진 랭킹 등 문제\n",
    "\n",
    "> - 가장 이상적인 랭킹(정답 랭킹)과 현재 점수를 활용한 랭킹사이의 점수를 cumulative하게 비교\n",
    "> - $1$에 가까울수록 좋은 랭킹\n",
    "> - $\\log_2{i}$로 normalization하여 순위가 낮을 수록 가중치를 감소\n",
    "> - 검색엔진, 영상, 음악 등 컨텐츠 랭킹 추천에서 주요 평가지표로 활용\n",
    "\n",
    "> - $$ CG_p = \\sum_{i=1}^p rel_i $$\n",
    "> - 상위 아이템 $p$개의 관련성을 합한 cumulative gain\n",
    "> - rel -> binary(관련 여부) 또는 complex value(문제에 따라 세분화된 값)\n",
    "> - 상위 아이템 $p$개에 대해서 동일한 비중으로 합함\n",
    "\n",
    "> - $$ DCG_p = rel_1 + \\sum_ {i=2}^p \\frac{rel_i}{\\log_2{i}} $$\n",
    "> - 개별 아이템의 관련성에 log normalization을 적용\n",
    "> - 랭킹에 따라 비중을 discount해서 관련성을 계산\n",
    "> - 하위권 penalty 부여\n",
    "\n",
    "> - $$ NDCG_p = \\frac{DCG_p}{IDCG_p} $$\n",
    "> - 이상적인 DCG(IDCG)를 계산하고, 최종 NDCG를 계산\n",
    "> - IDCG는 전체 $p$개의 결과 중 가질 수 있는 가장 큰 DCG\n",
    "\n",
    "### Other Evaluation Metrics\n",
    "\n",
    "> #### 1) Precision@K (Top-K)\n",
    "> - Top-K의 결과로 Precision 계산\n",
    "> - 관련 여부를 0 또는 1로 평가\n",
    "> - 예를 들어, 1,0,1,1,0일 때, Top-3는 $\\frac{2}{3}$, Top-5는 $\\frac{3}{5}$\n",
    "\n",
    "> #### 2) Mean Average Precision(MAP)\n",
    "> - 추천 랭킹 또는 검색 결과에 대한 average precision의 평균값 계산\n",
    "> - Precision@K($K_1,K_2,...,K_R$) -> 전체 Precision@K에 대한 평균값\n",
    "\n",
    "> #### Precision / Recalll, AUC\n",
    "> - 정밀도, 재현율\n",
    "> - 분류 문제의 정확도를 검증하고자 할 때 주로 사용되는 평가 지표"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
