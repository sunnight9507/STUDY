{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Multi-variable Linear Regression\n",
    "\n",
    "<img width=\"200\" src=\"https://i.imgur.com/hbPVe1T.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-variable linear regression\n",
    "\n",
    "> - Predicting exam score - regression using three inputs (x1, x2, x3)\n",
    "\n",
    "x1 (quiz 1) | x2 (quiz 2) | x3 (mid 1) | Y (final)\n",
    "---- | ---- | ----| ----\n",
    "73 | 80 | 75 | 152\n",
    "93 | 88 | 93 | 185\n",
    "89 | 91 | 90 | 180\n",
    "96 | 98 | 100 | 196\n",
    "73 | 66 | 70 | 142\n",
    "\n",
    "> - Test Scores for General Psychology ( https://goo.gl/g2T8Kp )\n",
    "\n",
    "# Matrix multiplication\n",
    "\n",
    "### dot product(=scalar product, 내적)\n",
    "\n",
    "<img src=\"https://www.mathsisfun.com/algebra/images/matrix-multiply-a.svg\" >\n",
    "\n",
    "[출처](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
    "\n",
    "# Multi-feature regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$$ H(x) = w x + b $$\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b $$\n",
    "\n",
    "# Hypothesis using matrix\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = \\underline{w_1 x_1 + w_2 x_2 + w_3 x_3} + b $$\n",
    "\n",
    "$$ \\Downarrow $$\n",
    "\n",
    "$$ \\begin{pmatrix} w_{1} & w_{2} & w_{3} \\end{pmatrix} \\cdot \\begin{pmatrix} x_{1} \\\\ x_{2} \\\\ x_{3} \\end{pmatrix}$$\n",
    "\n",
    "=> $ WX $ ($W$, $X$ 는 matrix)\n",
    "\n",
    "# Hypothesis without b\n",
    "\n",
    "$$ H(x_1, x_2, x_3) = w_1 x_1 + w_2 x_2 + w_3 x_3 + b$$\n",
    "\n",
    "$$ = b + w_1 x_1 + w_2 x_2 + w_3 x_3 $$\n",
    "\n",
    "$$ = \\begin{pmatrix} b & x_{ 1 } & x_{ 2 } & x_{ 3 } \\end{pmatrix}\\cdot \\begin{pmatrix} 1 \\\\ w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ = XW $$\n",
    "\n",
    "# Hypothesis using matrix \n",
    "\n",
    "### Many x instances\n",
    "\n",
    "$$ \\begin{pmatrix} x_{ 11 } & x_{ 12 } & x_{ 13 } \\\\ x_{ 21 } & x_{ 22 } & x_{ 23 } \\\\ x_{ 31 } & x_{ 32 } & x_{ 33 }\\\\ x_{ 41 } & x_{ 42 } & x_{ 43 }\\\\ x_{ 51 } & x_{ 52 } & x_{ 53 }\\end{pmatrix} \\cdot \\begin{pmatrix} w_{ 1 } \\\\ w_{ 2 } \\\\ w_{ 3 } \\end{pmatrix}=\\begin{pmatrix} x_{ 11 }w_{ 1 }+x_{ 12 }w_{ 2 }+x_{ 13 }w_{ 3 } \\\\ x_{ 21 }w_{ 1 }+x_{ 22 }w_{ 2 }+x_{ 23 }w_{ 3 }\\\\ x_{ 31 }w_{ 1 }+x_{ 32 }w_{ 2 }+x_{ 33 }w_{ 3 } \\\\ x_{ 41 }w_{ 1 }+x_{ 42 }w_{ 2 }+x_{ 43 }w_{ 3 } \\\\ x_{ 51 }w_{ 1 }+x_{ 52 }w_{ 2 }+x_{ 53 }w_{ 3 } \\end{pmatrix} $$\n",
    "\n",
    "$$ [5, 3] \\cdot [3, 1] = [5, 1] $$\n",
    "\n",
    "$$ H(X) = XW $$\n",
    "\n",
    "> - $5$는 데이터(instance)의 수, $3$은 변수(feature)의 수, $1$은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example (2 variables)\n",
    "\n",
    "x1 | x2 | y\n",
    "---- | ---- | ----\n",
    "1  |  0  |  1\n",
    "0  |  2  |  2\n",
    "3  |  0  |  3\n",
    "0  |  4  |  4\n",
    "5  |  0  |  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |  335.28082 |    -4.0663 |     1.1220 |  -6.065215\n",
      "  100 |   18.95926 |     0.7151 |     1.8781 |  -4.429109\n",
      "  200 |    3.44508 |     1.7284 |     2.0768 |  -3.961648\n",
      "  300 |    2.52540 |     1.9225 |     2.1184 |  -3.762738\n",
      "  400 |    2.33730 |     1.9403 |     2.1114 |  -3.629400\n",
      "  500 |    2.19633 |     1.9213 |     2.0881 |  -3.514729\n",
      "  600 |    2.06604 |     1.8953 |     2.0595 |  -3.407385\n",
      "  700 |    1.94368 |     1.8686 |     2.0293 |  -3.304398\n",
      "  800 |    1.82860 |     1.8425 |     1.9990 |  -3.204873\n",
      "  900 |    1.72033 |     1.8171 |     1.9693 |  -3.108468\n",
      " 1000 |    1.61847 |     1.7926 |     1.9403 |  -3.015011\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "\n",
    "x1_data = [1, 0, 3, 0, 5]\n",
    "x2_data = [0, 2, 0, 4, 0]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W1 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "W2 = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -10.0, 10.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.001)\n",
    "\n",
    "for i in range(1000 + 1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = W1 * x1_data + W2 * x2_data + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "    W1_grad, W2_grad, b_grad = tape.gradient(cost, [W1, W2, b])\n",
    "    W1.assign_sub(learning_rate * W1_grad)\n",
    "    W2.assign_sub(learning_rate * W2_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.5f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "          i, cost.numpy(), W1.numpy()[0], W2.numpy()[0], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example (2 variables with Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   9.711032 |     0.4891 |    -0.5854 |   0.813789\n",
      "  100 |   0.148628 |     0.7605 |     0.7157 |   0.911109\n",
      "  200 |   0.080663 |     0.8236 |     0.7906 |   0.671238\n",
      "  300 |   0.043778 |     0.8700 |     0.8458 |   0.494499\n",
      "  400 |   0.023759 |     0.9042 |     0.8864 |   0.364296\n",
      "  500 |   0.012895 |     0.9295 |     0.9163 |   0.268376\n",
      "  600 |   0.006998 |     0.9480 |     0.9383 |   0.197711\n",
      "  700 |   0.003798 |     0.9617 |     0.9546 |   0.145653\n",
      "  800 |   0.002061 |     0.9718 |     0.9665 |   0.107302\n",
      "  900 |   0.001119 |     0.9792 |     0.9753 |   0.079049\n",
      " 1000 |   0.000607 |     0.9847 |     0.9818 |   0.058235\n"
     ]
    }
   ],
   "source": [
    "x_data = [\n",
    "    [1., 0., 3., 0., 5.],\n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1, 2), -1.0, 1.0))\n",
    "b = tf.Variable(tf.random.uniform((1,), -1.0, 1.0))\n",
    "\n",
    "learning_rate = tf.Variable(0.01)\n",
    "\n",
    "for i in range(1000+1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data) + b # (1, 2) * (2, 5) = (1, 5)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.6f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], b.numpy()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis without b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0 |   7.723946 |    -0.9047 |     0.9896 |     0.0724\n",
      "  100 |   0.026807 |    -0.3870 |     1.1017 |     1.1206\n",
      "  200 |   0.014549 |    -0.2851 |     1.0749 |     1.0889\n",
      "  300 |   0.007896 |    -0.2100 |     1.0552 |     1.0655\n",
      "  400 |   0.004285 |    -0.1547 |     1.0407 |     1.0483\n",
      "  500 |   0.002326 |    -0.1140 |     1.0300 |     1.0355\n",
      "  600 |   0.001262 |    -0.0840 |     1.0221 |     1.0262\n",
      "  700 |   0.000685 |    -0.0619 |     1.0163 |     1.0193\n",
      "  800 |   0.000372 |    -0.0456 |     1.0120 |     1.0142\n",
      "  900 |   0.000202 |    -0.0336 |     1.0088 |     1.0105\n",
      " 1000 |   0.000110 |    -0.0247 |     1.0065 |     1.0077\n"
     ]
    }
   ],
   "source": [
    "# 앞의 코드에서 bias(b)를 행렬에 추가\n",
    "x_data = [\n",
    "    [1., 1., 1., 1., 1.], # bias(b)\n",
    "    [1., 0., 3., 0., 5.], \n",
    "    [0., 2., 0., 4., 0.]\n",
    "]\n",
    "y_data  = [1, 2, 3, 4, 5]\n",
    "\n",
    "W = tf.Variable(tf.random.uniform((1,3), -1.0, 1.0))\n",
    "\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "for i in range(1000 + 1):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis = tf.matmul(W, x_data)\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "\n",
    "    grads = tape.gradient(cost, [W])\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, [W]))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.6f} | {:10.4f} | {:10.4f} | {:10.4f}\".format(\n",
    "            i, cost.numpy(), W.numpy()[0][0], W.numpy()[0][1], W.numpy()[0][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Gradient\n",
    "* tf.train.GradientDescentOptimizer(): optimizer\n",
    "* optimizer.apply_gradients(): update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variable linear regression\n",
    "*  random  초기화: tf.random_normal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch | cost\n",
      "    0 |  4075.5820\n",
      "  100 |     6.6360\n",
      "  200 |     6.3900\n",
      "  300 |     6.1565\n",
      "  400 |     5.9350\n",
      "  500 |     5.7247\n",
      "  600 |     5.5250\n",
      "  700 |     5.3354\n",
      "  800 |     5.1554\n",
      "  900 |     4.9844\n",
      " 1000 |     4.8220\n",
      " 1100 |     4.6677\n",
      " 1200 |     4.5211\n",
      " 1300 |     4.3819\n",
      " 1400 |     4.2495\n",
      " 1500 |     4.1237\n",
      " 1600 |     4.0041\n",
      " 1700 |     3.8904\n",
      " 1800 |     3.7823\n",
      " 1900 |     3.6794\n",
      " 2000 |     3.5816\n"
     ]
    }
   ],
   "source": [
    "data = np.array([\n",
    "    # X1,   X2,    X3,   y\n",
    "    [ 73.,  80.,  75., 152. ],\n",
    "    [ 93.,  88.,  93., 185. ],\n",
    "    [ 89.,  91.,  90., 180. ],\n",
    "    [ 96.,  98., 100., 196. ],\n",
    "    [ 73.,  66.,  70., 142. ]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# slice data\n",
    "X = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "W = tf.Variable(tf.random.normal((3, 1)))\n",
    "b = tf.Variable(tf.random.normal((1,)))\n",
    "\n",
    "learning_rate = 0.00001\n",
    "\n",
    "# hypothesis, prediction function\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b\n",
    "\n",
    "print(\"epoch | cost\")\n",
    "\n",
    "n_epochs = 2000\n",
    "for i in range(n_epochs + 1):\n",
    "    # tf.GradientTape() to record the gradient of the cost function\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = tf.reduce_mean(tf.square(predict(X) - y))\n",
    "        \n",
    "    # calculates the gradients of the loss\n",
    "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
    "    \n",
    "    # updates parameters (W and b)\n",
    "    W.assign_sub(learning_rate * W_grad)\n",
    "    b.assign_sub(learning_rate * b_grad)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.663899  ],\n",
       "        [-0.04354759],\n",
       "        [ 1.3917944 ]], dtype=float32),\n",
       " array([-0.8917927], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.numpy(), b.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[148.4736 ],\n",
       "       [186.4555 ],\n",
       "       [179.49388],\n",
       "       [197.75429],\n",
       "       [142.1243 ]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[152.0, 185.0, 180.0, 196.0, 142.0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y # labels, 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[148.4736 ],\n",
       "       [186.4555 ],\n",
       "       [179.49388],\n",
       "       [197.75429],\n",
       "       [142.1243 ]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X).numpy() # 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182.10329],\n",
       "       [169.17186]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 새로운 데이터 예측\n",
    "\n",
    "predict([[ 89.,  95.,  92.],[ 84.,  92.,  85.]]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
